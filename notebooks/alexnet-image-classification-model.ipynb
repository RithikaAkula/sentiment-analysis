{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84cb027c",
   "metadata": {},
   "source": [
    "### IMPORT STATEMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9692b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "# from keras.optimizers import SGD, RMSprop, adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "from numpy import *\n",
    "from PIL import Image\n",
    "import theano\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numba\n",
    "from numba import jit, cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601ac5b",
   "metadata": {},
   "source": [
    "The sentences were presented using different emotion (in parentheses is the three letter code used in the third part of the filename):\n",
    "\n",
    "- Anger (ANG)\n",
    "- Disgust (DIS)\n",
    "- Fear (FEA)\n",
    "- Happy/Joy (HAP)\n",
    "- Neutral (NEU)\n",
    "- Sad (SAD)\n",
    "\n",
    "and emotion level (in parentheses is the two letter code used in the fourth part of the filename):\n",
    "\n",
    "- Low (LO)\n",
    "- Medium (MD)\n",
    "- High (HI)\n",
    "- Unspecified (XX)\n",
    "\n",
    "The suffix of the filename is based on the type of file, flv for flash video used for presentation of both the video only, and the audio-visual clips. mp3 is used for the audio files used for the audio-only presentation of the clips. wav is used for files used for computational audio processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d819b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_splits=\"D:/4-2/MAJOR_PROJECT/sentiment-analysis/dataset/split-dataset/\"\n",
    "SENTIMENTS = ['ANG','DIS','FEA','HAP','NEU','SAD']\n",
    "IMG_SIZE=277 # as required by AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f58769",
   "metadata": {},
   "source": [
    "#### create_dataset() RESIZES .JPG IMAGE, CREATES IMAGE ARRAY, AND EXTRACTS LABEL FROM IMAGE FILE NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57346508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(img_folder):\n",
    "    img_data_array=[]\n",
    "    class_ids=[]\n",
    "    for file in os.listdir(img_folder):\n",
    "\n",
    "        filename=file[:-4]\n",
    "        class_name=filename.split(\"_\")[2]\n",
    "        class_id=SENTIMENTS.index(class_name)\n",
    "\n",
    "        image_path= os.path.join(img_folder, file)\n",
    "        image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n",
    "        image=cv2.resize(image, (IMG_SIZE, IMG_SIZE),interpolation = cv2.INTER_AREA)\n",
    "        image=np.array(image)\n",
    "        image = image.astype('float32')\n",
    "        image /= 255 \n",
    "\n",
    "        img_data_array.append(image)\n",
    "        class_ids.append(class_id)\n",
    "\n",
    "    return img_data_array, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe8896f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the image array and class name\n",
    "test_images, test_labels = create_dataset(path_to_splits+'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb7811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_images, validation_labels = create_dataset(path_to_splits+'validation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6591b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = create_dataset(path_to_splits+'train/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b8ae7",
   "metadata": {},
   "source": [
    "### CONVERTING LABELS TO CATEGORICAL DATA\n",
    "\n",
    "#### current label is an integer from range 0 to 5, representing the index in the array SENTIMENTS. We need an array of length 6 to represent the output layer in the Neural Network\n",
    "the array has 1 to represent it's class in the SENTIMENTS array, and 0 for the rest\n",
    "\n",
    "eg: if current class_id = 2, then modified y_label will be [0. 0. 1. 0. 0. 0.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a29a989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels = np_utils.to_categorical(validation_labels, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09387953",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np_utils.to_categorical(test_labels, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427dac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np_utils.to_categorical(train_labels, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d985394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(745, 6)\n"
     ]
    }
   ],
   "source": [
    "print(shape(test_labels)) # (745, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51801486",
   "metadata": {},
   "source": [
    "### CONVERTING TO TensorFlow Dataset Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d014a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tfds(images_array, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((images_array, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "104c6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = convert_to_tfds(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1057a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ds = convert_to_tfds(validation_images, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db05c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = convert_to_tfds(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25879070",
   "metadata": {},
   "source": [
    "#### Get the size of the dataset partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59125818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5953\n",
      "Test data size: 745\n",
      "Validation data size: 744\n"
     ]
    }
   ],
   "source": [
    "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
    "print(\"Training data size:\", train_ds_size)\n",
    "print(\"Test data size:\", test_ds_size)\n",
    "print(\"Validation data size:\", validation_ds_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fef9bc",
   "metadata": {},
   "source": [
    "### PIPELINE to shuffle and batch the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a1840b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (train_ds\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "test_ds = (test_ds\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "validation_ds = (validation_ds\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ddc159",
   "metadata": {},
   "source": [
    "### ALEX-NET MODEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(4096, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e33f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d082a",
   "metadata": {},
   "source": [
    "### TENSORBOARD\n",
    "\n",
    "TensorBoard is a tool that provides a suite of visualization and monitoring mechanisms. Weâ€™ll be utilizing TensorBoard to monitor the progress of the training of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0204e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"logs\\\\fit\\\\\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41347bf7",
   "metadata": {},
   "source": [
    "### TRAINING AND RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c70a05",
   "metadata": {},
   "source": [
    "#### COMPILE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c6291d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 55, 55, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 27, 27, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              37752832  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58,327,818\n",
      "Trainable params: 58,325,066\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(learning_rate=0.001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb6e2f",
   "metadata": {},
   "source": [
    "#### TRAIN THE NETWORK\n",
    "\n",
    "Training the custom AlexNet network is very simple with the Keras module enabled through TensorFlow. We simply have to call the fit()method and pass relevant arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    model.fit(train_ds,\n",
    "            epochs=50,\n",
    "            validation_data=validation_ds,\n",
    "            validation_freq=1,\n",
    "            callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366aaad6",
   "metadata": {},
   "source": [
    "### EVALUATE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2f5f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
